{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv, random, os, sys, json, requests, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The onion list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onion = pd.read_csv(\"onion_path1\")\n",
    "\n",
    "# concat all the csv files\n",
    "# onion index 02 ~ 10\n",
    "for i in range(9):\n",
    "    onion_index = pd.read_csv(f\"onion_path{i+2}.csv\")\n",
    "    onion = pd.concat([onion, onion_index], ignore_index=True)\n",
    "\n",
    "onion = onion.sort_values(by='Date', ascending=False)\n",
    "onion = onion.dropna(subset=['Title', 'Content'])\n",
    "onion = onion[~onion['Content'].str.contains(\"Previous Slide Next Slide List slides\")]\n",
    "onion = onion[onion['Content'].str.count(\"Advertisement\") < 2]\n",
    "onion = onion.reset_index(drop=True)\n",
    "\n",
    "onion = onion.iloc[:1569, :] # for matching the number of data with reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1569.000000\n",
      "mean      942.959847\n",
      "std       491.174691\n",
      "min        66.000000\n",
      "25%       532.000000\n",
      "50%      1035.000000\n",
      "75%      1320.000000\n",
      "max      2544.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "onion_list = []\n",
    "for i in range(len(onion)):\n",
    "    onion_list.append('[Title]\\n' + onion.loc[i].to_list()[1] + '\\n' + '[Contents]\\n' + onion.loc[i].to_list()[3])\n",
    "\n",
    "onion_list_len = [len(x) for x in onion_list]\n",
    "onion_list_len = pd.Series(onion_list_len)\n",
    "print(onion_list_len.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reddit list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv('reddit_path1')\n",
    "reddit1 = pd.read_csv('reddit_path2', encoding='utf-8')\n",
    "reddit2 = pd.read_csv('reddit_path3', encoding='utf-8')\n",
    "reddit = pd.concat([reddit, reddit1, reddit2])\n",
    "reddit = reddit.dropna(subset=['Title', 'Content'])\n",
    "reddit = reddit.reset_index(drop=True)\n",
    "\n",
    "\n",
    "reddit_list = []\n",
    "for i in range(len(reddit)):\n",
    "    reddit_list.append('[Title]\\n' + reddit.loc[i].to_list()[1] + '\\n' + '[Contents]\\n' + reddit.loc[i].to_list()[3])\n",
    "\n",
    "# statistics\n",
    "reddit_list_len = [len(x) for x in reddit_list]\n",
    "reddit_list_len = [len(x) for x in reddit_list]\n",
    "reddit_list_len = pd.Series(reddit_list_len)\n",
    "print(reddit_list_len.describe())\n",
    "print(reddit.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of data: 3138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(len(onion_list)):\n",
    "    data.append([onion_list[i], 0])\n",
    "for i in range(len(reddit_list)):\n",
    "    data.append([reddit_list[i], 1])\n",
    "\n",
    "\n",
    "print(f'The length of data: {len(data)}')\n",
    "\n",
    "random.seed(43)\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPT4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompt_txt = \"\"\"The uploaded text is one of the articles that may be real or fake.\n",
    "Please Answer whether below article is fake or real. Give a 20-character rationale for why you think that way, and output a 0 and 1 at the end of the sentence.\n",
    "To Solve this, You have to think step by step. The first step in identifying fake news is evaluating the reliability of the information source. Well-known and verified news organizations are generally more reliable, and their reports can be trusted more than unverified sources.\n",
    "In addition to source reliability, look at the language used in the content. Fake news often uses sensational or exaggerated language designed to elicit an emotional response. It is also important to check for consistency and accuracy in the information presented; fake news typically includes claims that are either unverified or clearly false. Another critical step is cross-verification,\n",
    "where check if the same claims are reported by multiple trusted sources.\n",
    "i.e. rationale + answer 1 if you think the article is real, rationale + answer 0 if you think it is fake.\\n Must Keep in mind that the end of a sentence should end with either 0 or 1\n",
    "\n",
    "###\n",
    "Output: (rational) + (0||1)\n",
    "\"\"\"\n",
    "\n",
    "zeroshot_promt_txt = \"\"\"The uploaded text is one of the articles that may be real or fake.\n",
    "Please Answer whether below article is fake or real. Say nothing but the number 0 or 1.\n",
    "i.e. Answer 1 if you think the article is real, answer 0 if you think it is fake.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. GPT BatchAPI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"your_api_key\"\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original length of tasks: 3138\n",
      "The length of tasks: 3138\n",
      "The length of tasks: 3138\n"
     ]
    }
   ],
   "source": [
    "# Creating an array of json tasks\n",
    "tasks_zeroshot = []\n",
    "tasks_cot = []\n",
    "\n",
    "# task for zero-shot prompt\n",
    "for  i in range(len(data)):\n",
    "    task = {\n",
    "        \"custom_id\": f\"{i}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"temperature\": 0,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": zeroshot_promt_txt},\n",
    "                {\"role\": \"user\", \"content\": data[i][0]}\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    "    tasks_zeroshot.append(task)\n",
    "\n",
    "# task for cot prompt\n",
    "for i in range(len(data)):\n",
    "    task = {\n",
    "        \"custom_id\": f\"{i}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"temperature\": 0,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": cot_prompt_txt},\n",
    "                {\"role\": \"user\", \"content\": data[i][0]}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    tasks_cot.append(task)\n",
    "\n",
    "\n",
    "print(f\"The original length of tasks: {len(data)}\")\n",
    "print(f\"The length of tasks: {len(tasks_zeroshot)}\")\n",
    "print(f\"The length of tasks: {len(tasks_cot)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the file\n",
    "file_name_zeroshot = \"batch_tasks_zeroshot.jsonl\"\n",
    "with open(file_name_zeroshot, 'w') as file:\n",
    "    for obj in tasks_zeroshot:\n",
    "        file.write(json.dumps(obj) + '\\n')\n",
    "\n",
    "file_name_cot = \"batch_tasks_cot.jsonl\"\n",
    "with open(file_name_cot, 'w') as file:\n",
    "    for obj in tasks_cot:\n",
    "        file.write(json.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading the file\n",
    "batch_file_zeroshot = client.files.create(\n",
    "    file=open(file_name_zeroshot, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "batch_file_cot = client.files.create(\n",
    "    file=open(file_name_cot, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the batch job\n",
    "batch_job_zeroshot = client.batches.create(\n",
    "  input_file_id=batch_file_zeroshot.id,\n",
    "  endpoint=\"/v1/chat/completions\",\n",
    "  completion_window=\"24h\"\n",
    ")\n",
    "\n",
    "batch_job_cot = client.batches.create(\n",
    "  input_file_id=batch_file_cot.id,\n",
    "  endpoint=\"/v1/chat/completions\",\n",
    "  completion_window=\"24h\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the status of the batch\n",
    "batch_job_zeroshot = client.batches.retrieve(batch_job_zeroshot.id)\n",
    "print(batch_job_zeroshot.id)\n",
    "\n",
    "batch_job_cot = client.batches.retrieve(batch_job_cot.id)\n",
    "\n",
    "print(batch_job_zeroshot)\n",
    "print(batch_job_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retrieving the results\n",
    "# 1. zeroshot\n",
    "result_file_zeroshot_id = batch_job_zeroshot.output_file_id\n",
    "result_zeroshot = client.files.content(result_file_zeroshot_id).content\n",
    "result_zeroshot_file_name = \"data/batch_job_zeroshot_results.jsonl\"\n",
    "\n",
    "with open(result_zeroshot_file_name, 'w') as file:\n",
    "    file.write(result_zeroshot.decode('utf-8'))\n",
    "\n",
    "results_zeroshot = []\n",
    "with open(result_zeroshot_file_name, 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line.strip())\n",
    "        results_zeroshot.append(json_obj)\n",
    "\n",
    "\n",
    "\n",
    "# 2. cot\n",
    "result_file_cot_id = batch_job_cot.output_file_id\n",
    "print(result_file_cot_id)\n",
    "result_cot = client.files.content(result_file_cot_id).content\n",
    "result_cot_file_name = \"data/batch_job_cot_results.jsonl\"\n",
    "\n",
    "\n",
    "with open(result_cot_file_name, 'w') as file:\n",
    "    file.write(result_cot.decode('utf-8'))\n",
    "\n",
    "results_cot = []\n",
    "with open(result_cot_file_name, 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line.strip())\n",
    "        results_cot.append(json_obj)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statCV",
   "language": "python",
   "name": "statcv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
