{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv, random, os, sys, json, requests, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The onion list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onion = pd.read_csv(\"onion_path1\")\n",
    "\n",
    "# concat all the csv files\n",
    "# onion index 02 ~ 10\n",
    "for i in range(9):\n",
    "    onion_index = pd.read_csv(f\"onion_path{i+2}.csv\")\n",
    "    onion = pd.concat([onion, onion_index], ignore_index=True)\n",
    "\n",
    "onion = onion.sort_values(by='Date', ascending=False)\n",
    "onion = onion.dropna(subset=['Title', 'Content'])\n",
    "onion = onion[~onion['Content'].str.contains(\"Previous Slide Next Slide List slides\")]\n",
    "onion = onion[onion['Content'].str.count(\"Advertisement\") < 2]\n",
    "onion = onion.reset_index(drop=True)\n",
    "\n",
    "onion = onion.iloc[:1569, :] # for matching the number of data with reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onion_list = []\n",
    "for i in range(len(onion)):\n",
    "    onion_list.append('[Title]\\n' + onion.loc[i].to_list()[1] + '\\n' + '[Contents]\\n' + onion.loc[i].to_list()[3])\n",
    "\n",
    "\n",
    "onion_list_len = [len(x) for x in onion_list]\n",
    "onion_list_len = pd.Series(onion_list_len)\n",
    "print(onion_list_len.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reddit list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv('reddit_path1')\n",
    "reddit1 = pd.read_csv('reddit_path2', encoding='utf-8')\n",
    "reddit2 = pd.read_csv('reddit_path3', encoding='utf-8')\n",
    "reddit = pd.concat([reddit, reddit1, reddit2])\n",
    "reddit = reddit.dropna(subset=['Title', 'Content'])\n",
    "reddit = reddit.reset_index(drop=True)\n",
    "\n",
    "\n",
    "reddit_list = []\n",
    "for i in range(len(reddit)):\n",
    "    reddit_list.append('[Title]\\n' + reddit.loc[i].to_list()[1] + '\\n' + '[Contents]\\n' + reddit.loc[i].to_list()[3])\n",
    "\n",
    "# statistics\n",
    "reddit_list_len = [len(x) for x in reddit_list]\n",
    "reddit_list_len = [len(x) for x in reddit_list]\n",
    "reddit_list_len = pd.Series(reddit_list_len)\n",
    "print(reddit_list_len.describe())\n",
    "print(reddit.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(onion_list)):\n",
    "    data.append([onion_list[i], 0])\n",
    "for i in range(len(reddit_list)):\n",
    "    data.append([reddit_list[i], 1])\n",
    "\n",
    "\n",
    "print(f'The length of data: {len(data)}')\n",
    "\n",
    "random.seed(43)\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPT4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"The uploaded text is one of the articles that may be real or fake.\n",
    "Please Answer whether below article is fake or real. Give a 20-character rationale for why you think that way, and output a 0 and 1 at the end of the sentence.\n",
    "To Solve this, You have to think step by step. The first step in identifying fake news is evaluating the reliability of the information source. Well-known and verified news organizations are generally more reliable, and their reports can be trusted more than unverified sources.\n",
    "In addition to source reliability, look at the language used in the content. Fake news often uses sensational or exaggerated language designed to elicit an emotional response. It is also important to check for consistency and accuracy in the information presented; fake news typically includes claims that are either unverified or clearly false. Another critical step is cross-verification,\n",
    "where check if the same claims are reported by multiple trusted sources.\n",
    "i.e. rationale + answer 1 if you think the article is real, rationale + answer 0 if you think it is fake.\\n Must Keep in mind that the end of a sentence should end with either 0 or 1\n",
    "\n",
    "###\n",
    "Output: (rational) + (0||1)\n",
    "\"\"\"\n",
    "\n",
    "fewshot = \"\"\"Examples:\n",
    "\n",
    "Example Article (Real)\n",
    "Title: \"Three women contract HIV from dirty “vampire facials” at unlicensed spa\"\\\\\n",
    "Content: \"Trendy, unproven 'vampire facials' performed at an unlicensed spa in New Mexico left at least three women with HIV infections. \n",
    "This marks the first time that cosmetic procedures have been associated with an HIV outbreak, \n",
    "according to a detailed report of the outbreak investigation published today. Ars reported on the cluster last year when state health \n",
    "officials announced they were still identifying cases linked to the spa despite it being shut down in September 2018. \n",
    "But today’s investigation report offers more insight into the unprecedented outbreak, which linked five people with HIV infections to the spa and spurred investigators to contact and test nearly 200 other spa clients.\"\\\\\n",
    "Example Article (Fake)\\\\\n",
    "Title: \"New NFL Combine Drill Tests Player’s Ability To Half-Ass Taping Of Local Sandwich Shop Commercial\"\n",
    "Content: \"INDIANAPOLIS—In an effort to gain a fuller picture of prospective players and ensure they would be good fits for the teams that drafted them,\n",
    " a new NFL Scouting Combine drill tested a player’s ability to half-ass a taping of a local sandwich shop commercial,\n",
    "  sources confirmed Monday. As part of our new drill, invited players have been asked to stand stiffly in a jersey and ill-fitting jeans while reading awkwardly from cue cards that encourage customers to come on down to Big Dave’s or Galaxy Sandwich.\\\\\n",
    "  \n",
    "rationale: When determining the authenticity of a news story, \"Three women contract HIV from dirty 'vampire facials' at unlicensed spa\" is judged to be real news because it is based on detailed investigative reporting and clear facts from a trusted source 1 . On the other hand, \"New NFL Combine Drill Tests Player's Ability To Half-Ass Taping Of Local Sandwich Shop Commercial\" is considered fake news because it uses satirical language and an unrealistic claim that the NFL Combine tests players' ability to film commercials 0.  \n",
    "\n",
    "\n",
    "Now, Answer whether below article is fake or real with format below.\n",
    "Output: (rational) + (0||1)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"your_api_key\"\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_fewshot = []\n",
    "\n",
    "# task for cot prompt\n",
    "for i in range(len(data)):\n",
    "    task = {\n",
    "        \"custom_id\": f\"{i}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"temperature\": 0,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": fewshot + data[i][0]}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    tasks_fewshot.append(task)\n",
    "\n",
    "print(f\"The length of tasks_fewshot: {len(tasks_fewshot)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the file\n",
    "\n",
    "file_name_fewshot = \"batch_fewshot_cot.jsonl\"\n",
    "\n",
    "with open(file_name_fewshot, 'w') as file:\n",
    "    for obj in tasks_fewshot:\n",
    "        file.write(json.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading the file\n",
    "\n",
    "batch_file_fewshot = client.files.create(\n",
    "    file=open(file_name_fewshot, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the batch job\n",
    "batch_job_fewshot = client.batches.create(\n",
    "    input_file_id=batch_file_fewshot.id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the status of the batch job\n",
    "batch_job_fewshot = client.batches.retrieve(batch_job_fewshot.id) # \n",
    "client.batches.ret\n",
    "print(batch_job_fewshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_fewshot_id = batch_job_fewshot.output_file_id\n",
    "print(f\"Fewshot file id: {result_file_fewshot_id}\")\n",
    "\n",
    "result_fewshot = client.files.content(result_file_fewshot_id).content\n",
    "result_fewshot_file_name = \"data/batch_job_fewshot_results.jsonl\"\n",
    "\n",
    "with open(result_fewshot_file_name, \"w\") as file:\n",
    "    file.write(result_fewshot.decode('utf-8'))\n",
    "    \n",
    "results_fewshot = []\n",
    "with open(result_fewshot_file_name, 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line.strip())\n",
    "        results_fewshot.append(json_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether there is a case that the last character is not 0 or 1\n",
    "error_case = []\n",
    "\n",
    "for i in range(len(results_fewshot)):\n",
    "    res = results_fewshot[i]['response']['body']['choices'][0]['message']['content']\n",
    "    if res[-1] != '0' and res[-1] != '1':\n",
    "        if res[-2] != '0' and res[-2] != '1':\n",
    "            error_case.append(i)\n",
    "\n",
    "assert len(error_case) == 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
